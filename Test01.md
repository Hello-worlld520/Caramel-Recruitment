# 焦糖招新test01

* 多层感知机是一种前馈神经网络，信息从输入层开始逐层向前传递，直到输出层，没有循环或反馈连接，结构：
  * 输入层
  * 隐藏层：可能不止一层，一层的每一个神经元都与前一层的每一个神经元相连接（全连接)，每个隐藏层都会从前一层的输出中提取更抽象更高级的特征
  * 输出层

* 数据相当于神经网络的学习资料

  数据集的划分

  * 训练集：训练模型，调整模型自身的参数
  * 验证集：像高考前的一二三模，调整神经网络的学习率、网络层数、模型架构、停止训练等
  * 测试集：最终检验模型

  数据处理：数据需要经过一定处理后再喂给模型，数据处理方式有归一化，处理缺失值，数据增强

  噪声是意外，个人理解为高中物理中的错误和误差，能避免的尽量避免，能降低的尽量降低

  特征是输入的数据的各项信息

  标签是正确输出，有的数据有标签，有的数据无标签，取决于我想要进行监督学习还是无监督学习

  ***batch size***

  在训练神经网络时，每次输入模型并计算一次梯度更新所使用的样本数量

  ***why batch size***

  GPU具有并行计算能力，擅长对大量数据进行批量化处理，在有同样计算量的情况下，把有相同计算需求的数据打包成一份能加快速度，还能不让GPU有闲着的部分

  就是依据硬件特性匹配合适的计算形式

* 神经元是MLP+激活函数

  ![image-20250908233357065](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250908233357065.png)

  张量:张量是多维数组

* 激活函数是一个作用在神经元输出结果上的一个非线性函数

  常见的激活函数：

  * **Sigmoid**

    ![image-20250909173421846](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250909173421846.png)

    ![image-20250909173446601](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250909173446601.png)

    梯度会在两边消失

  * **Tanh**

    ![image-20250914152133685](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250914152133685.png)

    仍有梯度消失问题

  * **ReLU**

    ![image-20250914161322143](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250914161322143.png)

    常用于隐藏层，能保持梯度不消失，但可能有部分输入小于0导致死神经元，造成网络稀疏

  * **Leaky ReLU**

    ![image-20250914162301382](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250914162301382.png)

    将0改为0.1x，避免死亡神经元问题

  * **Softmax**

    ![image-20250914163117031](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250914163117031.png)

    相当于把输出结果转化为0-1之间的数字，这个数字代表概率，选取概率最大的那个，softmax常用于多分类输出层中

  - [x] ==TODO==学习对应的激活函数并整理、贴图


***激活函数的选择***

* 隐藏层有relu选择relu，如果发现神经元死亡的问题换用leaky-relu，不要使用sigmoid函数，容易梯度消失，可以尝试tanh函数

  输出层：

  * 二分类问题选择sigmoid函数
  * 多酚类问题选择softmax函数
  * 回归问题选择identity函数

如果没有激活函数，那神经网络只能进行线性的计算，激活函数使y=ax+b有了微调的能力，能把原本是直线的函数掰弯，不同的激活函数就是不同的掰法，也是激活函数使神经网络能够进行相对复杂而非单一的决策，使之具有非线性表达能力（不是单一累加或递减，而是能够综合参考多种因素的能力)

* 计算图是用节点表示变量或者计算操作，用边表示数据流动的一种有向图

  数据结构和离散数学中的图节点表示固定的实体或者常量，边表示节点之间的关系，没有节点随边流动改变的感觉

  - [ ] 动态图的构建
  - [ ] 静态图的构建

* MLP的参数量：计算模型中所有需要学习的权重和偏置的总和

  一层神经网络的参数量计算方法：

  参数量 = (输入特征数 + 1) × 输出特征数

  总参数量是各层神经网络参数量的加和

  超参数是在开始学习之前由开发者手动设置的配置参数，控制模型的训练过程和整体结构

  MLP的超参数

  * 网络结构超参数
    * 隐藏层数量
    * 每层的神经元数量
    * 激活函数
  * 优化器超参数
    * 学习率
    * 批量大小
    * 优化算法
    * 迭代次数：通常不直接设置，使用'早停技术'
  * 正则化超参数
    * L1/L2正则化强度
    * Dropout比率:随机暂时忽略一部分神经元
    * 早停的耐心值：决定在验证集性能不再提升后，允许继续训练的轮数

* 隐藏层位于输入层和输出层之间，是抽象的，不可见的，所以被称为隐藏层
  * 全连接隐藏层
  * 卷积隐藏层
  * 循环隐藏层
  * 注意力隐藏层
    - [ ] bi网课

* ***什么是损失函数？什么任务用什么损失函数？***

  损失函数是衡量模型预测结果与真实结果之间差异的函数，最小最佳

* ***前向传播是什么？梯度是什么？学习率是什么？反向传播是什么？有哪些常见的优化器？***

  * 前向传播是神经网络从输入层到输出层的数据流动过程，通过各层的权重和激活函数计算最终输出。

  * 梯度是损失函数对每个参数的偏导数，本质是一个向量，表示函数在某个点增长最快的方向及其增长率

  * 学习率控制每次参数更新的步长大小，太小了训练时间过长，太大了震荡或不收敛

  * 反向传播是通过链式法则从输出层到输入层计算梯度的算法。

  * 常见的优化器：

    * BGD![image-20250915114904825](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250915114904825.png)

    * SGD
    * Momentum

  - [ ] ==弄清原理和具体公式==

* ***归一化是什么？正则化是什么？***

  归一化是将数据缩放到特定范围的过程，能够消除数据间的量纲差异，使不同数据具有可比性

  正则化是通过在损失函数中添加惩罚项来防止模型过拟合的技术

  - [ ] ==具体的正则化和归一化技术==

* ***什么是欠拟合？什么是过拟合？***

  过拟合是指模型在训练数据上表现的很好，但在未见过的数据上测试成绩很差，是由于模型过度学习了训练数据中的噪声和细节，泛化能力下降（学习过于死板，同样题型变个数字就不会了）

  欠拟合是指模型过于简单，在训练数据和测试数据上都表现不好（压根没学）
  
  # 焦糖招新task01代码实现
  
  ==线索式计划==
  
  - [x] python类复习
  - [ ] pytorch的使用
  
  - [ ] Kaggle免费GPU使用教程B站
  
  #免费GPU使用超详细教程 Kaggle注册使用技巧 深度学习同学的福音 白嫖的快乐 TensorFlow PyTorch均可使用
  
  `nn.Sequential能够用于按顺序定义网络层，适合简单的线性网络结构
  
  ***pytorch learning***
  
  卷积层：把一张图像转化为0-1之间的矩阵,粗提取输入数据的特征
  
  池化层：压缩矩阵，精细提取数据特征
  
  全连接层：将更小的图像展开成一段数据
  
  * 准备数据
  
    * 数据下载
  
      URL：网址/链接
  
    * 数据格式转换
  
    * 数据集划分
  
  * 简单的数字识别模型
  
    * 网课截图
  
  ![image-20250918213722022](C:\Users\ThinkPad\Desktop\Typora笔记\image-20250918213722022.png)
  
  ​		 自己实践
  
  ```python
  from torch.nn import Module
  class CNN(Module):#定义一个名字叫做CNN的类，继承自torch.nn.Module
      def _init_(self,n_channels):
          
      
  ```
  
  
  
  
